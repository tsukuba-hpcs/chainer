start:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
start:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
start:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
start:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
finish:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
finish:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
finish:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
finish:data = comm.bcast_obj(data, max_buf_len=max_buf_len, root=0)
start:extensions initializer
finish:extensions initializer
start:update()
start:batch = iterator.next()
start:extensions initializer
finish:extensions initializer
start:update()
start:batch = iterator.next()
start:extensions initializer
finish:extensions initializer
start:update()
start:batch = iterator.next()
start:extensions initializer
finish:extensions initializer
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.bcast_data(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.bcast_data(target)
start:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.bcast_data(target)
start:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
start:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.bcast_data(target)
start:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
end:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
end:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
end:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
end:nccl_comm_id = mpi_comm.bcast(nccl_comm_id)
finish:self.communicator.bcast_data(target)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.communicator.bcast_data(target)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.communicator.bcast_data(target)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.communicator.bcast_data(target)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:update()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
finish <chainer.training.extensions.print_report.PrintReport object at 0x2b945b824358> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:batch = iterator.next()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:update()
start:batch = iterator.next()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:batch = iterator.next()
finish:update()
finish:update()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
finish:batch = iterator.next()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
finish:batch = iterator.next()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
finish:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:batch = iterator.next()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
finish:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
finish:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start:update()
start:batch = iterator.next()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
start:update()
finish:optimizer.update(loss_func, *in_arrays)
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:batch = iterator.next()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
finish:update()
finish:update()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:batch = iterator.next()
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
start:update()
finish:optimizer.update(loss_func, *in_arrays)
start:update()
start:batch = iterator.next()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:update()
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
finish:update()
start:update()
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
finish:batch = iterator.next()
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
<class 'chainermn.communicators.pure_nccl_communicator.PureNcclCommunicator'>
start:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.communicator.allreduce_grad(target)
finish:self.communicator.allreduce_grad(target)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
start:self.actual_optimizer.update(None, *args, **kwds)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:update()
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
finish:self.actual_optimizer.update(None, *args, **kwds)
finish:optimizer.update(loss_func, *in_arrays)
start <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
finish:optimizer.update(loss_func, *in_arrays)
finish:update()
finish:update()
finish:update()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
start:update()
start:batch = iterator.next()
finish <chainer.training.extensions.log_report.LogReport object at 0x2b945b824278> extension
start <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
finish <chainer.training.extensions.progress_bar.ProgressBar object at 0x2b945b8244a8> extension
start:update()
start:batch = iterator.next()
finish:batch = iterator.next()
start:optimizer.update(loss_func, *in_arrays) <class 'chainermn.optimizers._MultiNodeOptimizer'>
